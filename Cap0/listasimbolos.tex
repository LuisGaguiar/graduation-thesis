\begin{longtable}{ll}
$\mathbb{A}$ & A set \\
$\mathbb{R}$ & The set of real numbers \\

\\
$a_i$ & Element $i$ of vector $a$, with indexing starting at 1 \\
$A^T$ & Transpose of matrix $A$ \\
$\frac{dy}{dx}$ & Derivative of $y$ with respect to $x$\\
$\frac{\partial y}{\partial x}$ & Partial derivative of $y$ with respect to $x$ \\
$\nabla y$ & Gradient of $y$ \\
$\int_a^bf(x)dx $ & Definite integral with respect to x over [a, b] \\
$\mathbb{I}^{\text{condition}}$ & conditional unit function \\

\\
$f:\mathbb{A}\to\mathbb{B}$ & The function $f$ with domain $\mathbb{A}$ and range $\mathbb{B}$ \\
$f(x,\theta)$ & A function of $x$ parameterized by $\theta$

\\
$\mathbb{P}(x)$ & Probability distribution over a continuous variable \\
$\mathbb{E}[X]$ & Expectation of random variable $X$ \\
KL$[P,Q]$       & KL divergence between P and Q \\
$\log{x}$ & Natural logarithm of $x$ \\


\\ \textbf{Reinforcement Learning} \\
% TODO RL Section
$s, s\prime$ & States \\
$a$ & Action \\
$r$ & Reward \\
$t$ & Discrete timestep \\
$A_t$ & Action at timestep $t$ \\
$S_t$ & State at timestep $t$ \\
$R_t$ & Reward at timestep $t$ \\
$G_t$ & Return (cumulative discounted reward) following time $t$ \\ 

\\
$\pi$ & Policy, agent behavior rule \\
$\pi(s)$ &  Action taken in state s under policy $\pi$ \\

\\
$v_\pi(s)$ &  Value of state s under policy $\pi$ (expected return) \\
$v_*(s)$ &  Value of state s under the optimal policy  \\
$q_\pi(s,a)$ &  Value of taking action $a$ in state $s$ under policy $\pi$ \\
$q_\pi(s,a)$ &  Value of taking action $a$ state $s$ under policy $\pi$ \\
$V, V_t$     &  Estimate of state-value function $v_\pi$ or $v_*$\\
$Q, Q_t$     &  Estimate of action-value function $q_\pi$ or $q_*$\\
$ $     &  \\

\end{longtable}

